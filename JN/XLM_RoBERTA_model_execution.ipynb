{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ca9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs # https://github.com/ThilinaRajapakse/simpletransformers\n",
    "import pandas as pd\n",
    "import logging\n",
    "import torch\n",
    "#import torchvision\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b38de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(number_of_runs, classification_filename, model_savedir, model_type):\n",
    "\n",
    "    with open(classification_filename, 'r', encoding=\"utf-8\") as file:\n",
    "        dataset = file.read().splitlines()\n",
    "    for index, d in enumerate(dataset):\n",
    "        dataset[index] = d.split(';')\n",
    "        dataset[index][1] = dataset[index][1].strip()\n",
    "        #dataset[index][1] = dataset[index][1].lower()\n",
    "    \n",
    "    training_group_size = int(len(dataset) * 0.7)\n",
    "    eval_group_size = int((len(dataset) - training_group_size) / 2)\n",
    "    prediction_group_size = len(dataset) - training_group_size - eval_group_size\n",
    "\n",
    "    print(f\"Total length of dataset: {len(dataset)}\")\n",
    "    print(f\"Length of training group: {training_group_size}\")\n",
    "    print(f\"Length of eval group: {eval_group_size}\")\n",
    "    print(f\"Length of prediction group: {prediction_group_size}\")\n",
    "    print('')\n",
    "    \n",
    "    for i in range(number_of_runs):\n",
    "        datadir = model_savedir + str(i) + \"/\"\n",
    "\n",
    "        # Optional model configuration\n",
    "        model_args = prepare_options(datadir, training_group_size, model_type)\n",
    "        print(model_args)\n",
    "\n",
    "        # Training session if no model is present in this dir\n",
    "        if not os.path.isdir(datadir):\n",
    "            print(f'Doing new prediction in {datadir}')\n",
    "            training_session(i, dataset, datadir, model_args, training_group_size, eval_group_size, prediction_group_size)\n",
    "            \n",
    "        # Full prediction afterwards or if model is already present\n",
    "        full_predictions(dataset, datadir, training_group_size, eval_group_size, prediction_group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e71dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_options(datadir, training_group, model_type):\n",
    "    model_args = ClassificationArgs()\n",
    "    if model_type == \"cookie dialog\":\n",
    "        model_args.num_train_epochs = 5\n",
    "        model_args.labels_list = [\"True\", \"False\"]\n",
    "        model_args.max_seq_length = 512\n",
    "    elif model_type == \"buttons\":\n",
    "        model_args.num_train_epochs = 20\n",
    "        model_args.labels_list = [\"ACCEPT\", \"DECLINE\", \"MODIFY\", \"SAVE\", \"OTHER\"]\n",
    "        model_args.max_seq_length = 64\n",
    "        \n",
    "    model_args.output_dir = datadir\n",
    "    model_args.cache_dir = datadir + \"cache/\"\n",
    "\n",
    "    model_args.learning_rate = 2e-05\n",
    "    model_args.train_batch_size = 32\n",
    "    model_args.eval_batch_size = 32\n",
    "    model_args.max_seq_length = 64\n",
    "    model_args.encoding = 'utf-8'\n",
    "    model_args.evaluate_during_training = True\n",
    "    model_args.evaluate_during_training_verbose = True\n",
    "    model_args.use_multiprocessing_for_evaluation = False\n",
    "    model_args.evaluate_each_epoch = True\n",
    "    model_args.do_lower_case = True\n",
    "    model_args.save_model_every_epoch = False\n",
    "    model_args.save_steps = -1\n",
    "    model_args.save_eval_checkpoints = False\n",
    "    model_args.best_model_dir = datadir + \"best_model/\"\n",
    "    model_args.save_best_model = True\n",
    "\n",
    "    # Early stopping metric\n",
    "    model_args.use_early_stopping = True\n",
    "    model_args.early_stopping_delta = 0.001\n",
    "    model_args.early_stopping_metric = \"mcc\"\n",
    "    model_args.early_stopping_metric_minimize = False\n",
    "    model_args.early_stopping_patience = 5\n",
    "    model_args.evaluate_during_training_steps = int(training_group / model_args.train_batch_size / 3 * 4)\n",
    "\n",
    "    return model_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ecf8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_session(i, dataset, datadir, model_args, training_group_size, eval_group_size, prediction_group_size):\n",
    "    dataset_shuffle = random.sample(dataset, len(dataset))\n",
    "    print(f'Start training {i}')\n",
    "\n",
    "    # Preparing train data\n",
    "    train_data = []\n",
    "    for d in dataset_shuffle[:training_group_size]:\n",
    "        train_data.append([d[3].lower(), d[4]])\n",
    "\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    train_df.columns = [\"text\", \"labels\"]\n",
    "\n",
    "    # Preparing eval data\n",
    "    eval_data = []\n",
    "    for d in dataset_shuffle[training_group_size:training_group_size + eval_group_size]:\n",
    "        eval_data.append([d[3].lower(), d[4]])\n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "    eval_df.columns = [\"text\", \"labels\"]\n",
    "\n",
    "    print(f'No model preset, doing training {i}')\n",
    "\n",
    "    # Create a ClassificationModel\n",
    "    model = ClassificationModel(\n",
    "        \"xlmroberta\", \"xlm-roberta-base\", num_labels=len(model_args.labels_list),\n",
    "        args=model_args, use_cuda=False\n",
    "    )  # FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(train_df, eval_df=eval_df)\n",
    "    print('--------------model trained--------------------')\n",
    "\n",
    "\n",
    "    print('--------------Doing predictions on small dataset--------------------')\n",
    "\n",
    "    # Make predictions with the model\n",
    "    make_prediction(dataset_shuffle, model, training_group_size, eval_group_size, prediction_group_size)\n",
    "\n",
    "    print('--------------prediction made on small dataset--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5dd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(dataset, model, training_group_size, eval_group_size, prediction_group_size):\n",
    "    # Prepare predictions for the dataset\n",
    "    pred_data = []\n",
    "    res_data = []\n",
    "    for d in dataset[training_group_size + eval_group_size:training_group_size + eval_group_size + prediction_group_size]:\n",
    "        pred_data.append(d[0].lower())\n",
    "        res_data.append(d[1])\n",
    "\n",
    "    # Do predictions\n",
    "    predictions, raw_outputs = model.predict(pred_data)\n",
    "\n",
    "    # Analyse results\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    for index in range(len(pred_data)):\n",
    "        # print(predictions[index], end=\" - \")\n",
    "        # print(res_data[index])\n",
    "        # results.append([predictions[index], res_data[index]])\n",
    "        if res_data[index] == predictions[index]:\n",
    "            right += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "            print(index, end=\" - \")\n",
    "            print(pred_data[index], end=\" - \")\n",
    "            print(res_data[index], end=\" - \")\n",
    "            print(predictions[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c5f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_predictions(dataset, datadir, training_group_size, eval_group_size, prediction_group_size):\n",
    "    print('--------------Doing predictions on full dataset--------------------')\n",
    "\n",
    "    # Reuse model\n",
    "    model = ClassificationModel(\"xlmroberta\", datadir, use_cuda=False)\n",
    "\n",
    "    # Prepare predictions for the whole dataset\n",
    "    make_prediction(dataset, model, training_group_size, eval_group_size, prediction_group_size)\n",
    "\n",
    "    print('--------------prediction made on full dataset--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ab7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    " #   main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6d375a6e2c1dd629f8ae9029f5d025192cb72fe1211813c90f07a61aadb7945"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
