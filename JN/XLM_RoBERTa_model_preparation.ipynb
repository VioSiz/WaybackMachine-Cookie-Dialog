{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "873e5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run global_vars.ipynb\n",
    "%run XLM_RoBERTa_model_execution.ipynb\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d093551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Cookie dialog model\n",
      "2: Buttons model model\n",
      "Which model would you like to train? 2\n",
      "How many runs do you want to do? 1\n"
     ]
    }
   ],
   "source": [
    "print('1: Cookie dialog model')\n",
    "print('2: Buttons model model')\n",
    "choice = int(input('Which model would you like to train? '))\n",
    "choice_runs = int(input('How many runs do you want to do? '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd03971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of dataset: 1155\n",
      "Length of training group: 808\n",
      "Length of eval group: 173\n",
      "Length of prediction group: 174\n",
      "\n",
      "ClassificationArgs(adafactor_beta1=None, adafactor_clip_threshold=1.0, adafactor_decay_rate=-0.8, adafactor_eps=(1e-30, 0.001), adafactor_relative_step=True, adafactor_scale_parameter=True, adafactor_warmup_init=True, adam_betas=(0.9, 0.999), adam_epsilon=1e-08, best_model_dir='V:/Uni/Thesis/Code/ThesisWaybackMachine-Cookie-Dialog//buttons_model0/best_model/', cache_dir='V:/Uni/Thesis/Code/ThesisWaybackMachine-Cookie-Dialog//buttons_model0/cache/', config={}, cosine_schedule_num_cycles=0.5, custom_layer_parameters=[], custom_parameter_groups=[], dataloader_num_workers=0, do_lower_case=True, dynamic_quantize=False, early_stopping_consider_epochs=False, early_stopping_delta=0.001, early_stopping_metric='mcc', early_stopping_metric_minimize=False, early_stopping_patience=5, encoding='utf-8', eval_batch_size=32, evaluate_during_training=True, evaluate_during_training_silent=True, evaluate_during_training_steps=33, evaluate_during_training_verbose=True, evaluate_each_epoch=True, fp16=True, gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, loss_type=None, loss_args={}, manual_seed=None, max_grad_norm=1.0, max_seq_length=64, model_name=None, model_type=None, multiprocessing_chunksize=-1, n_gpu=1, no_cache=False, no_save=False, not_saved_args=[], num_train_epochs=20, optimizer='AdamW', output_dir='V:/Uni/Thesis/Code/ThesisWaybackMachine-Cookie-Dialog//buttons_model0/', overwrite_output_dir=False, polynomial_decay_schedule_lr_end=1e-07, polynomial_decay_schedule_power=1.0, process_count=6, quantized_model=False, reprocess_input_data=True, save_best_model=True, save_eval_checkpoints=False, save_model_every_epoch=False, save_optimizer_and_scheduler=True, save_steps=-1, scheduler='linear_schedule_with_warmup', silent=False, skip_special_tokens=True, tensorboard_dir=None, thread_count=None, tokenizer_name=None, tokenizer_type=None, train_batch_size=32, train_custom_parameters_only=False, use_cached_eval_features=False, use_early_stopping=True, use_hf_datasets=False, use_multiprocessing=True, use_multiprocessing_for_evaluation=False, wandb_kwargs={}, wandb_project=None, warmup_ratio=0.06, warmup_steps=0, weight_decay=0.0, model_class='ClassificationModel', labels_list=['ACCEPT', 'DECLINE', 'MODIFY', 'SAVE', 'OTHER'], labels_map={}, lazy_delimiter='\\t', lazy_labels_column=1, lazy_loading=False, lazy_loading_start_line=1, lazy_text_a_column=None, lazy_text_b_column=None, lazy_text_column=0, onnx=False, regression=False, sliding_window=False, special_tokens_list=[], stride=0.8, tie_value=1)\n",
      "Doing new prediction in V:/Uni/Thesis/Code/ThesisWaybackMachine-Cookie-Dialog//buttons_model0/\n",
      "Start training 0\n",
      "No model preset, doing training 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\marie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:486: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8d3f1747a843ec864c835846b0b152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9315053ee0c54f0dbf2721bf0cf0ae74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 20:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if choice == 1:\n",
    "    number_of_runs = choice_runs\n",
    "    classification_filename = classification_dialog_filename\n",
    "    model_savedir = model_savedir_dialog\n",
    "    model_type = \"cookie dialog\"\n",
    "elif choice == 2:\n",
    "    number_of_runs = choice_runs\n",
    "    classification_filename = classification_buttons_filename\n",
    "    model_savedir = model_savedir_buttons\n",
    "    model_type = \"buttons\"\n",
    "\n",
    "main(number_of_runs, classification_filename, model_savedir, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "639c2a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: 'V:/Uni/Thesis/Code/Thesis/buttons_model0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_savedir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: 'V:/Uni/Thesis/Code/Thesis/buttons_model0'"
     ]
    }
   ],
   "source": [
    "os.remove(model_savedir + '0')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6d375a6e2c1dd629f8ae9029f5d025192cb72fe1211813c90f07a61aadb7945"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
