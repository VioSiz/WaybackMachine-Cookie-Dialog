{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from waybackpy import WaybackMachineCDXServerAPI\n",
    "from tranco import Tranco\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_NUMBER_OF_WEBSITES = 1000\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) Gecko/20100101 Firefox/105.0\"\n",
    "START_DATE_RANGE = 201604\n",
    "END_DATE_RANGE = 202104\n",
    "\n",
    "DATE_TRANCO_LIST='2019-02-20' # Date for fetching Tranco list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_french_websites(top_n=TOTAL_NUMBER_OF_WEBSITES):\n",
    "    \n",
    "    # Initializing tranco-list.eu list.\n",
    "    tranco_client = Tranco(cache=True, cache_dir='.tranco')\n",
    "\n",
    "    # Get tranco list for the specific date\n",
    "    list_for_date = tranco_client.list(date=DATE_TRANCO_LIST)\n",
    "\n",
    "    # Filter the list to what we are looking for.\n",
    "    french_websites = [web for web in list_for_date.list if '.fr' in web]\n",
    "        \n",
    "    return french_websites[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84e0af-7493-46c0-ae78-fb0cc097925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_archived_urls(website, start_date, end_date, collapse_by, depth = 0):\n",
    "    \"\"\"\n",
    "    Fetches archived URLs for a specific website within a date range using Wayback Machine API.\n",
    "    \"\"\"\n",
    "    # Abort if recursive depth exceeds 2\n",
    "    if depth > 2:\n",
    "        return []\n",
    "    \n",
    "    archived_urls = []\n",
    "    # Initialize Wayback Machine API with parameters\n",
    "    cdx_api = WaybackMachineCDXServerAPI(\n",
    "        url=website, \n",
    "        user_agent=USER_AGENT, \n",
    "        start_timestamp=start_date,\n",
    "        end_timestamp=end_date, \n",
    "        collapses=[collapse_by]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Iterate through snapshots and extract URL and date\n",
    "        for snapshot in cdx_api.snapshots():\n",
    "            date_of_snapshot = snapshot.archive_url.split('/')[4][:8]\n",
    "            archived_urls.append((snapshot.archive_url, date_of_snapshot))\n",
    "    except Exception as e:\n",
    "        # If an error occurs, retry after a delay, incrementing the recursion depth\n",
    "        sleep(30)\n",
    "        return fetch_archived_urls(website, start_date, end_date, collapse_by, depth+1)\n",
    "    return archived_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f4e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_websites(websites):\n",
    "    \"\"\"\n",
    "    Processes a list of websites, retrieving archived URLs for each.\n",
    "    \"\"\"\n",
    "    processed_data = {}\n",
    "\n",
    "    # Loop through each website and retrieve its snapshots\n",
    "    for website in tqdm(websites):\n",
    "        processed_data[website] = fetch_archived_urls(website, START_DATE_RANGE, END_DATE_RANGE, \"timestamp:6\")\n",
    "        sleep(20) # Throttle requests to avoid API rate limits\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81466c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createJSON(filename, dictionary):\n",
    "    # Store JSON log\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(dictionary, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_of_urls(websites):\n",
    "    \"\"\"\n",
    "    Create a dictionary of URLs and dates, and store it in a JSON file.\n",
    "    \"\"\"\n",
    "    dictionary_of_urls = process_websites(websites)\n",
    "    \n",
    "    # Store the dictionary in a JSON file\n",
    "    createJSON(\"dictionary_of_urls_1000.json\", dictionary_of_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c810674",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    french_websites = fetch_french_websites()\n",
    "    create_dictionary_of_urls(french_websites)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d799dc2d536255115a19d6a8c8812415a92ff2ed5c345a8603c5fb3a59d0fca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
