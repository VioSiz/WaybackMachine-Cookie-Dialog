{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import glob, os, os.path\n",
    "\n",
    "from waybackpy import WaybackMachineCDXServerAPI\n",
    "from time import sleep, time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from tranco import Tranco\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_NUMBER_OF_WEBSITES = 1000\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) Gecko/20100101 Firefox/105.0\"\n",
    "START_DATE_RANGE = 201604\n",
    "END_DATE_RANGE = 202104\n",
    "\n",
    "\n",
    "DATE_TRANCO_LIST='2019-02-20' # Date for fetching Tranco list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_french_websites(top_n=TOTAL_NUMBER_OF_WEBSITES):\n",
    "    \n",
    "    # Initializing tranco-list.eu list.\n",
    "    tranco_client = Tranco(cache=True, cache_dir='.tranco')\n",
    "\n",
    "    # Get tranco list for the specific date\n",
    "    list_for_date = tranco_client.list(date=DATE_TRANCO_LIST)\n",
    "\n",
    "    # Filter the list to what we are looking for.\n",
    "    french_websites = [web for web in list_for_date.list if '.fr' in web]\n",
    "        \n",
    "    return french_websites[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84e0af-7493-46c0-ae78-fb0cc097925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_archived_urls(website, start_date, end_date, collapse_by, depth = 0):\n",
    "    # if we try it more than 2 times we abort\n",
    "    if depth > 1:\n",
    "        return []\n",
    "    \n",
    "    archived_urls = []\n",
    "    \n",
    "    cdx_api = WaybackMachineCDXServerAPI(url=website, user_agent=USER_AGENT, start_timestamp=start_date, end_timestamp=end_date, collapses=[collapse_by])\n",
    "    snapshots = cdx_api.snapshots()\n",
    "    \n",
    "    try:\n",
    "        for snapshot in snapshots:\n",
    "            date_of_snapshot = snapshot.archive_url.split('/')[4][:8]\n",
    "            archived_urls.append((snapshot.archive_url, date_of_snapshot))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sleep(30)\n",
    "        return fetch_archived_urls(website, start_date, end_date, collapse_by, depth+1)\n",
    "    return archived_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae29eda-c4a3-4050-888d-d24bd9dfa88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_over_months(website):\n",
    "    # list of urls for the website for each month\n",
    "    \n",
    "    urls_of_dates = fetch_archived_urls(website, START_DATE_RANGE, END_DATE_RANGE, \"timestamp:6\")\n",
    "    \n",
    "    print(website, len(urls_of_dates))\n",
    "    return (urls_of_dates, len(urls_of_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f4e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_websites(websites):\n",
    "    processed_data = {}\n",
    "\n",
    "    for website in tqdm(websites):\n",
    "        processed_data[website] = fetch_archived_urls(website, START_DATE_RANGE, END_DATE_RANGE, \"timestamp:6\")\n",
    "        sleep(20)\n",
    "    print(f\"Processed: {len(processed_data)} URLs\")\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print URLs nicely\n",
    "def print_dictionary_of_urls(dictory_of_urls):\n",
    "    for website, urls in dictory_of_urls.items():\n",
    "        print(f\"Website: {website}\")\n",
    "        for url, date in urls:\n",
    "            print(f\"URL: {url}, Date: {date}\")\n",
    "        print()\n",
    "\n",
    "#print_dictionary_of_urls(dictory_of_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81466c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createJSON(filename, dictionary):\n",
    "    # Store JSON log\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(dictionary, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81645570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remJSON():\n",
    "    mydir = r\"V:\\Uni\\Thesis\\Code\\Thesis\"\n",
    "    filelist = glob.glob(os.path.join(mydir, \"*.JSON\"))\n",
    "    for f in filelist:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_of_urls(websites):\n",
    "    \"\"\"\n",
    "    Create a dictionary of URLs and dates, and store it in a JSON file.\n",
    "    \"\"\"\n",
    "    dictionary_of_urls = process_websites(websites)\n",
    "    \n",
    "    # Store the dictionary in a JSON file\n",
    "    createJSON(\"dictionary_of_urls_1000.json\", dictionary_of_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95273b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_dictionary_of_urls(fetch_french_websites())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d1ec80-ab5c-47ad-ad16-e0ca98916824",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a2b3d-f5b8-4fed-8db5-5a9d10247a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary_of_urls_1000.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "valid = True\n",
    "\n",
    "for web in data:\n",
    "    for link in data[web]:\n",
    "        valid&= web in link\n",
    "        if not web in link[0].lower():\n",
    "            print(link, web)\n",
    "            break\n",
    "print(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f939544f-30fa-4f3b-9932-ecc394725a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
