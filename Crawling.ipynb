{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ee0adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "from waybackpy import *\n",
    "import glob, os, os.path\n",
    "\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from tranco import Tranco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09e4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_option = ['NOTHING', 'ACCEPT', 'REJECT', 'MANAGE']\n",
    "GLOBAL_SELECTOR = \"a, button, div, span, form, p\"\n",
    "NUMBER_OF_WEBSITES = 3\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) Gecko/20100101 Firefox/105.0\"\n",
    "START_DATE_FRANCE = datetime.date(2020,7, 14)\n",
    "END_DATE_FRANCE = datetime.date(2020, 12, 25)\n",
    "delta = datetime.timedelta(days=31)\n",
    "\n",
    "DATE_TRANCO_LIST='2022-10-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "521a015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['www.free.fr', 'www.google.fr', 'www.amazon.fr']\n"
     ]
    }
   ],
   "source": [
    "# Initializing tranco-list.eu list.\n",
    "t = Tranco(cache=True, cache_dir='.tranco')\n",
    "\n",
    "# Get tranco list for the specific date\n",
    "date_list = t.list(date=DATE_TRANCO_LIST)\n",
    "\n",
    "# Filter the list to what we are looking for.\n",
    "french_websites = [web for web in date_list.list if '.fr' in web]\n",
    "french_websites_www = [f'{website}' for website in french_websites]\n",
    "\n",
    "unformatted_websites = french_websites_www[0:NUMBER_OF_WEBSITES]\n",
    "\n",
    "first_three_websites = []\n",
    "for website in unformatted_websites:\n",
    "    if not website.startswith(\"www.\"):\n",
    "        website = \"www.\" + website\n",
    "    first_three_websites.append(website)\n",
    "    \n",
    "print(first_three_websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bce4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date format to integer value (ymd)\n",
    "def to_integer(dt_time):\n",
    "    return 10000 * dt_time.year + 100 * dt_time.month + dt_time.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6dce459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of urls for crawling\n",
    "def waybackify_url(url, closest_timestamp):\n",
    "    return f'https://web.archive.org/web/{closest_timestamp}/{url}'\n",
    "\n",
    "loist = {}\n",
    "\n",
    "for i in first_three_websites:\n",
    "    loist2 = []\n",
    "    start_date = START_DATE_FRANCE\n",
    "    while start_date <= END_DATE_FRANCE:\n",
    "        url = waybackify_url(i, to_integer(start_date))\n",
    "        loist2.append((url, (start_date.year, start_date.month)))\n",
    "        start_date += delta\n",
    "    loist[i] = loist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b220531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over list of months for a specific website\n",
    "def go_over_months(website):\n",
    "    # list of urls for the website for each month\n",
    "    urls_of_dates = []\n",
    "    wayback_obj = Url(website).save()\n",
    "    #print(wayback_obj)\n",
    "\n",
    "    # iterating between 01-07-2019 to 25-03-25 by jumps of 31 days \n",
    "    start_date = START_DATE_FRANCE\n",
    "\n",
    "    # append list of urls using wayback obect wuth specific date to interact with the API.\n",
    "    while start_date <= END_DATE_FRANCE:\n",
    "        # sometimes the API fails so in that case we simply try again\n",
    "        try:\n",
    "            archive_url = wayback_obj.near(year=start_date.year, month=start_date.month).archive_url\n",
    "            urls_of_dates.append((archive_url, (start_date.year, start_date.month)))\n",
    "            print(website, archive_url, \"Success.\")\n",
    "            # go to the next month\n",
    "            start_date += delta\n",
    "            # the API is rather slow so we need to give some time to rest (:\n",
    "            sleep(3)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "            \n",
    "    return urls_of_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "798e5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each website, visit urls of each months.\n",
    "def go_over_websites(websites_list):\n",
    "    get_list = {}\n",
    "    for url in websites_list:\n",
    "        get_list[url] = go_over_months(url)  \n",
    "    return get_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3cf5172",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.free.fr https://web.archive.org/web/20170616092025/http://www2004.free.fr/ Success.\n",
      "www.free.fr https://web.archive.org/web/20170616092025/http://www2004.free.fr/ Success.\n",
      "www.free.fr https://web.archive.org/web/20170616092025/http://www2004.free.fr/ Success.\n",
      "www.free.fr https://web.archive.org/web/20170616092025/http://www2004.free.fr/ Success.\n",
      "www.free.fr https://web.archive.org/web/20170616092025/http://www2004.free.fr/ Success.\n",
      "www.free.fr https://web.archive.org/web/20170616092025/http://www2004.free.fr/ Success.\n",
      "www.google.fr https://web.archive.org/web/20200729230705/https://www.google.fr/ Success.\n",
      "www.google.fr https://web.archive.org/web/20200830010830/http://www.google.fr/ Success.\n",
      "www.google.fr https://web.archive.org/web/20200929234438/http://www.google.fr/ Success.\n",
      "www.google.fr https://web.archive.org/web/20201030002813/https://www.google.fr/ Success.\n",
      "www.google.fr https://web.archive.org/web/20201129235416/https://www.google.fr/ Success.\n",
      "www.google.fr https://web.archive.org/web/20201229232241/https://www.google.fr/ Success.\n",
      "www.amazon.fr https://web.archive.org/web/20200729234242/https://www.amazon.fr/ Success.\n",
      "{\"url\": \"www.amazon.fr\", \"archived_snapshots\": {}, \"timestamp\": \"202008300010\"}\n",
      "{\"url\": \"www.amazon.fr\", \"archived_snapshots\": {}, \"timestamp\": \"202008300010\"}\n",
      "{\"url\": \"www.amazon.fr\", \"archived_snapshots\": {}, \"timestamp\": \"202008300010\"}\n",
      "{\"url\": \"www.amazon.fr\", \"archived_snapshots\": {}, \"timestamp\": \"202008300010\"}\n",
      "{\"url\": \"www.amazon.fr\", \"archived_snapshots\": {}, \"timestamp\": \"202008300010\"}\n",
      "{\"url\": \"www.amazon.fr\", \"archived_snapshots\": {}, \"timestamp\": \"202008300010\"}\n",
      "{\"url\": \"www.amazon.fr\", \"archived_snapshots\": {}, \"timestamp\": \"202008300010\"}\n",
      "www.amazon.fr https://web.archive.org/web/20200830002959/https://www.amazon.fr/ Success.\n",
      "www.amazon.fr https://web.archive.org/web/20200929230941/https://www.amazon.fr/ Success.\n",
      "www.amazon.fr https://web.archive.org/web/20201030010014/https://www.amazon.fr/ Success.\n",
      "www.amazon.fr https://web.archive.org/web/20201129205741/https://www.amazon.fr/ Success.\n",
      "www.amazon.fr https://web.archive.org/web/20201230014548/https://www.amazon.fr/ Success.\n",
      "{'www.free.fr': [('https://web.archive.org/web/20170616092025/http://www2004.free.fr/', (2020, 7)), ('https://web.archive.org/web/20170616092025/http://www2004.free.fr/', (2020, 8)), ('https://web.archive.org/web/20170616092025/http://www2004.free.fr/', (2020, 9)), ('https://web.archive.org/web/20170616092025/http://www2004.free.fr/', (2020, 10)), ('https://web.archive.org/web/20170616092025/http://www2004.free.fr/', (2020, 11)), ('https://web.archive.org/web/20170616092025/http://www2004.free.fr/', (2020, 12))], 'www.google.fr': [('https://web.archive.org/web/20200729230705/https://www.google.fr/', (2020, 7)), ('https://web.archive.org/web/20200830010830/http://www.google.fr/', (2020, 8)), ('https://web.archive.org/web/20200929234438/http://www.google.fr/', (2020, 9)), ('https://web.archive.org/web/20201030002813/https://www.google.fr/', (2020, 10)), ('https://web.archive.org/web/20201129235416/https://www.google.fr/', (2020, 11)), ('https://web.archive.org/web/20201229232241/https://www.google.fr/', (2020, 12))], 'www.amazon.fr': [('https://web.archive.org/web/20200729234242/https://www.amazon.fr/', (2020, 7)), ('https://web.archive.org/web/20200830002959/https://www.amazon.fr/', (2020, 8)), ('https://web.archive.org/web/20200929230941/https://www.amazon.fr/', (2020, 9)), ('https://web.archive.org/web/20201030010014/https://www.amazon.fr/', (2020, 10)), ('https://web.archive.org/web/20201129205741/https://www.amazon.fr/', (2020, 11)), ('https://web.archive.org/web/20201230014548/https://www.amazon.fr/', (2020, 12))]}\n"
     ]
    }
   ],
   "source": [
    "dictory_of_urls = go_over_websites(first_three_websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1f4b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dictionary_of_urls(dictory_of_urls):\n",
    "    for website, urls in dictory_of_urls.items():\n",
    "        print(f\"Website: {website}\")\n",
    "        for url, date in urls:\n",
    "            print(f\"URL: {url}, Date: {date}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b32025ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website: www.free.fr\n",
      "URL: https://web.archive.org/web/20170616092025/http://www2004.free.fr/, Date: (2020, 7)\n",
      "URL: https://web.archive.org/web/20170616092025/http://www2004.free.fr/, Date: (2020, 8)\n",
      "URL: https://web.archive.org/web/20170616092025/http://www2004.free.fr/, Date: (2020, 9)\n",
      "URL: https://web.archive.org/web/20170616092025/http://www2004.free.fr/, Date: (2020, 10)\n",
      "URL: https://web.archive.org/web/20170616092025/http://www2004.free.fr/, Date: (2020, 11)\n",
      "URL: https://web.archive.org/web/20170616092025/http://www2004.free.fr/, Date: (2020, 12)\n",
      "\n",
      "Website: www.google.fr\n",
      "URL: https://web.archive.org/web/20200729230705/https://www.google.fr/, Date: (2020, 7)\n",
      "URL: https://web.archive.org/web/20200830010830/http://www.google.fr/, Date: (2020, 8)\n",
      "URL: https://web.archive.org/web/20200929234438/http://www.google.fr/, Date: (2020, 9)\n",
      "URL: https://web.archive.org/web/20201030002813/https://www.google.fr/, Date: (2020, 10)\n",
      "URL: https://web.archive.org/web/20201129235416/https://www.google.fr/, Date: (2020, 11)\n",
      "URL: https://web.archive.org/web/20201229232241/https://www.google.fr/, Date: (2020, 12)\n",
      "\n",
      "Website: www.amazon.fr\n",
      "URL: https://web.archive.org/web/20200729234242/https://www.amazon.fr/, Date: (2020, 7)\n",
      "URL: https://web.archive.org/web/20200830002959/https://www.amazon.fr/, Date: (2020, 8)\n",
      "URL: https://web.archive.org/web/20200929230941/https://www.amazon.fr/, Date: (2020, 9)\n",
      "URL: https://web.archive.org/web/20201030010014/https://www.amazon.fr/, Date: (2020, 10)\n",
      "URL: https://web.archive.org/web/20201129205741/https://www.amazon.fr/, Date: (2020, 11)\n",
      "URL: https://web.archive.org/web/20201230014548/https://www.amazon.fr/, Date: (2020, 12)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_dictionary_of_urls(dictory_of_urls)\n",
    "\n",
    "# To do: validate this list by checking\n",
    "# wheter every link for a specific website \"contains\" the original name of this website starting with \"www.\". \n",
    "# Print a separate json file with the check results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c396a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for website in dictory_of_urls:\n",
    "       website_name = website\n",
    "       print(website)\n",
    "       for url in dictory_of_urls[website]:\n",
    "           print(url[1])\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium Firefox setup\n",
    "options = FirefoxOptions()\n",
    "# options.headless = True\n",
    "service = FirefoxService(r'C:\\Program Files\\Mozilla Firefox\\geckodriver.exe')\n",
    "driver = webdriver.Firefox(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f94617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Collect cookies for every month, write to two JSON file.\n",
    "    for website in loist:\n",
    "        website_name = website\n",
    "        for url in loist[website]:\n",
    "            print(website, url[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1379ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81645570",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydir = r\"V:\\Uni\\Thesis\\Code\\Thesis\"\n",
    "filelist = glob.glob(os.path.join(mydir, \"*.JSON\"))\n",
    "for f in filelist:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81466c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store JSON log\n",
    "with open(\"{}_all_urls.txt\".format(\"Urls\"), 'w') as outfile:\n",
    "                json.dump(dictory_of_urls, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_banner(driver):\n",
    "    accept_words_list = set()\n",
    "    for w in open(\"accept_words_languages.txt\", \"r\", encoding=\"utf-8\").read().splitlines():\n",
    "        if not w.startswith(\"#\") and not w == \"\":\n",
    "            accept_words_list.add(w)\n",
    "\n",
    "    sleep(20)\n",
    "    content = driver.find_elements(By.CSS_SELECTOR, GLOBAL_SELECTOR)\n",
    "    for element in content:\n",
    "        try:\n",
    "            if element.text.lower().strip(\" ✓›!\\n\") in accept_words_list:\n",
    "                print(element.text)\n",
    "                print(\"and here\")\n",
    "                #WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, driver.get(element)))).click()\n",
    "                element.click()\n",
    "                return {'successful': True, 'error': 'none'}\n",
    "        except Exception as e:\n",
    "            print(\"Failed\", e)\n",
    "            return {'successful': False, 'error': 'click_error'}\n",
    "    return {'successful': False, 'error': 'no_accept_button' }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
