{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53c0dd-6388-483b-bb60-b4f549f74522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from selenium.common.exceptions import WebDriverException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61218f19-5d11-45ab-b835-6e7db87e89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Model_setup.ipynb\n",
    "%run Create_list_of_URLs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce73cb9-99fa-417f-b761-8e8cd6b3ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_url(url):\n",
    "    \"\"\"\n",
    "    Crawl a URL, extract elements, and pass them to an ML model.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the web page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up Firefor WebDriver\n",
    "        driver = setup_firefox_driver()\n",
    "        \n",
    "        # Visit the URL and retrieve page source\n",
    "        driver.get(url)\n",
    "        \n",
    "        sleep(5)\n",
    "        html = driver.page_source\n",
    "        \n",
    "        # Initialize a list to store cookies\n",
    "        list_cookies = []\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Find iframes and active elements\n",
    "        iframes = soup.find_all(['div', 'iframe'])\n",
    "        active_elements = soup.find_all(['a', 'button', 'input', 'yt-formatted-string', 'p', 'span', 'form'])\n",
    "\n",
    "        #wait until we passed the wayback redirect \n",
    "        while any(\"Got an HTTP 302 response at crawl time\" in str(s) for s in active_elements):\n",
    "            sleep(5)\n",
    "            html = driver.page_source\n",
    "    \n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "            # Find iframes and active elements\n",
    "            iframes = soup.find_all(['div', 'iframe'])\n",
    "            active_elements = soup.find_all(['a', 'button', 'input', 'yt-formatted-string', 'p', 'span', 'form'])\n",
    "\n",
    "        sleep(15)\n",
    "\n",
    "        html = driver.page_source\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Find iframes and active elements\n",
    "        active_elements = soup.find_all(['a', 'button', 'input', 'yt-formatted-string', 'p', 'span', 'form'])\n",
    "        iframes = []\n",
    "        for x in soup.find_all(['div', 'iframe']):\n",
    "            iframes.append((x,0))\n",
    "\n",
    "        # Find button elements\n",
    "        button_elements = soup.find_all(['button', 'input[type=\"submit\"]', 'a'])\n",
    "\n",
    "        # Extract text content from active elements\n",
    "        text_content = extract_text_from_elements(active_elements)\n",
    "\n",
    "        # Extract button text\n",
    "        button_text = extract_text_from_elements(button_elements)\n",
    "        #text_content.extend(extract_text_from_iframes(iframes, driver))\n",
    "\n",
    "        # Search within iframes\n",
    "        search_frames(iframes, text_content, button_text)\n",
    "\n",
    "        # Get cookies from the page\n",
    "        list_cookies = driver.get_cookies()\n",
    "        driver.quit()\n",
    "\n",
    "        # Put the text that has the word cookie first as it is more likely to be a cookie dialog\n",
    "        text_content_containing_cookies = [text for text in text_content if \"cookie\" in text]\n",
    "        text_content = text_content_containing_cookies + text_content\n",
    "\n",
    "        \n",
    "        \n",
    "        # Pass text content to the ML model for dialog detection\n",
    "        dialog_result = pass_to_ml_model_dialog(text_content)\n",
    "        \n",
    "        if dialog_result != \"not found\":\n",
    "            return (dialog_result, pass_to_ml_model_buttons(button_text)), list_cookies, \"found\"\n",
    "        return (), list_cookies, \"not found\"\n",
    "    \n",
    "    except (WebDriverException, TimeoutException) as e:\n",
    "        return (), [], \"error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aaaa1a-82fc-48fd-ba9d-aa93e6200ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_website_data(websites_to_crawl, dictionary_of_urls):\n",
    "    \"\"\"\n",
    "    Collect data from websites including URLs, dates, results, and cookies.\n",
    "\n",
    "    Args:\n",
    "        dictionary_of_urls (dict): Dictionary of URLs with associated dates.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing collected data.\n",
    "    \"\"\"\n",
    "    \n",
    "    collected_data = {} # Initialize a dictionary to store cookies\n",
    "    \n",
    "    # Crawl each website and store the results\n",
    "    for web in websites_to_crawl:\n",
    "        try_saved = {}\n",
    "        try_url = \"\"\n",
    "        collected_data[web] = {}\n",
    "        dialogues_found = 0\n",
    "        for url_to_visit, date in tqdm(dictionary_of_urls[web]):\n",
    "            if dialogues_found <= 3:\n",
    "                result_data, cookies, found = crawl_url(url_to_visit)\n",
    "                if found == \"found\":\n",
    "                    dialogues_found = 0\n",
    "                else:\n",
    "                    dialogues_found+=1\n",
    "                print(\"I did {} cookie dialog and the val is {}\".format(found, dialogues_found))\n",
    "                if not dialogues_found:\n",
    "                    if try_saved:\n",
    "                        if not try_saved[\"results dialog\"] == result_data[0]:\n",
    "                            collected_data[web][try_url] = try_saved\n",
    "                    try_saved = {\n",
    "                        \"date\"   : date,\n",
    "                        \"results dialog\": result_data[0],\n",
    "                        \"results buttons\": result_data[1],\n",
    "                        \"cookies\": cookies\n",
    "                    }\n",
    "                    try_url = url_to_visit\n",
    "                if found == \"error\" :\n",
    "                    print(f\"Error while crawling URL: {url_to_visit} - {result_data}\")\n",
    "                    dialogues_found = 0\n",
    "                    collected_data[web][url_to_visit] = \"error\"\n",
    "                else:\n",
    "                    print(f\"Successfully cralwed URL: {url_to_visit}\")\n",
    "            else:\n",
    "                break\n",
    "        if try_url:\n",
    "            collected_data[web][try_url] = try_saved\n",
    "        else: \n",
    "            collected_data[web] = \"no dialog found\"\n",
    "    return collected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631586f-e152-4f4b-8cea-d31f06b2f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read the dictionary_of_urls from the JSON file\n",
    "    with open(\"dictionary_of_urls_1000.json.json\", \"r\") as json_file:\n",
    "        loaded_dictionary = json.load(json_file)    \n",
    "    \n",
    "    websites_to_visit = list(loaded_dictionary)\n",
    "    for web in websites_to_visit:\n",
    "        loaded_dictionary[web].reverse()\n",
    "    \n",
    "    collected_data = collect_website_data(websites_to_visit, loaded_dictionary)\n",
    "    \n",
    "    # Save the collected data to a JSON file\n",
    "    createJSON(\"collected_data.json\", collected_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
