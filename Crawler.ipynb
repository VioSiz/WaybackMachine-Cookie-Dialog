{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53c0dd-6388-483b-bb60-b4f549f74522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from selenium.common.exceptions import WebDriverException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61218f19-5d11-45ab-b835-6e7db87e89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run text_and_button_classifier.ipynb\n",
    "%run Create_list_of_URLs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce73cb9-99fa-417f-b761-8e8cd6b3ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_url(url):\n",
    "    \"\"\"\n",
    "    Crawls a webpage, extracts HTML elements, and passes content to the ML model for classification.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the web page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up Firefor WebDriver\n",
    "        driver = setup_firefox_driver()\n",
    "        \n",
    "        # Visit the URL and retrieve page source\n",
    "        driver.get(url)\n",
    "        sleep(5)\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find iframes and active elements\n",
    "        iframes = soup.find_all(['div', 'iframe'])\n",
    "        active_elements = soup.find_all(['a', 'button', 'input', 'yt-formatted-string', 'p', 'span', 'form'])\n",
    "\n",
    "        #Handle Wayback Machine redirects, if encountered \n",
    "        while any(\"Got an HTTP 302 response at crawl time\" in str(s) for s in active_elements):\n",
    "            sleep(5)\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            # Find iframes and active elements\n",
    "            iframes = soup.find_all(['div', 'iframe'])\n",
    "            active_elements = soup.find_all(['a', 'button', 'input', 'yt-formatted-string', 'p', 'span', 'form'])\n",
    "\n",
    "        sleep(15)\n",
    "        \n",
    "        # Refresh HTML elements post-load\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        active_elements = soup.find_all(['a', 'button', 'input', 'yt-formatted-string', 'p', 'span', 'form'])\n",
    "\n",
    "        # Update iframe elements as tuples with depth information for frame navigation\n",
    "        iframes = [(x, 0) for x in soup.find_all(['div', 'iframe'])]\n",
    "\n",
    "        # Find button elements\n",
    "        button_elements = soup.find_all(['button', 'input[type=\"submit\"]', 'a'])\n",
    "\n",
    "        # Extract text content from active and button elements\n",
    "        text_content = extract_text_from_elements(active_elements)\n",
    "        button_text = extract_text_from_elements(button_elements)\n",
    "\n",
    "        # Additional text search within iframes\n",
    "        search_frames(iframes, text_content, button_text)\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        # Prioritize elements likely containing the word \"cookie\" for efficient processing\n",
    "        text_content_containing_cookies = [text for text in text_content if \"cookie\" in text]\n",
    "        text_content = text_content_containing_cookies + text_content\n",
    "\n",
    "        # Classify the gathered content with ML model\n",
    "        dialog_result = pass_to_ml_model_dialog(text_content)\n",
    "        \n",
    "        if dialog_result != \"not found\":\n",
    "            return (dialog_result, pass_to_ml_model_buttons(button_text)), \"found\"\n",
    "        return (), \"not found\"\n",
    "    \n",
    "    except (WebDriverException, TimeoutException) as e:\n",
    "        # Handle driver-related or page load errors\n",
    "        return (), \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aaaa1a-82fc-48fd-ba9d-aa93e6200ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_website_data(websites_to_crawl, dictionary_of_urls):\n",
    "    \"\"\"\n",
    "    Collect data from websites including URLs, dates, and results.\n",
    "\n",
    "    Args:\n",
    "        websites_to_crawl (list): List of website domains to crawl.\n",
    "        dictionary_of_urls (dict): Dictionary where each key is a website domain, \n",
    "                                   and the value is a list of tuples containing URLs and dates.\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing collected data, including dialogue results and timestamps.\n",
    "    \"\"\"\n",
    "    \n",
    "    collected_data = {} # Initialize a dictionary to store cookies\n",
    "    \n",
    "    # Crawl each website and store the results\n",
    "    for web in websites_to_crawl:\n",
    "        try_saved = {}      # Temporary storage for the latest successful crawl result\n",
    "        try_url = \"\"        # URL of the latest successful crawl\n",
    "        collected_data[web] = {}  # Initialize storage for each websiteâ€™s crawled data\n",
    "        dialogues_found = 0  # Counter for consecutive non-dialogue pages\n",
    "\n",
    "        # Crawl each URL for the website with its associated date\n",
    "        for url_to_visit, date in tqdm(dictionary_of_urls[web]):\n",
    "            if dialogues_found <= 3:\n",
    "                result_data, found = crawl_url(url_to_visit)\n",
    "\n",
    "                # Reset counter if a dialogue is found\n",
    "                if found == \"found\":\n",
    "                    dialogues_found = 0\n",
    "                else:\n",
    "                    dialogues_found+=1\n",
    "\n",
    "                # Save the result if dialogue is found\n",
    "                if not dialogues_found:\n",
    "                    # Store the result if it's the first or a new result\n",
    "                    if try_saved and try_saved[\"results dialog\"] != result_data[0]:\n",
    "                        collected_data[web][try_url] = try_saved        \n",
    "                    try_saved = {\n",
    "                        \"date\"   : date,\n",
    "                        \"results dialog\": result_data[0],\n",
    "                        \"results buttons\": result_data[1]\n",
    "                    }\n",
    "                    try_url = url_to_visit\n",
    "\n",
    "                # Log errors\n",
    "                if found == \"error\" :\n",
    "                    dialogues_found = 0\n",
    "                    collected_data[web][url_to_visit] = \"error\"\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Finalize data collection for the current website  \n",
    "        if try_url:\n",
    "            collected_data[web][try_url] = try_saved\n",
    "        else: \n",
    "            collected_data[web] = \"no dialog found\" #  No dialogue found for this website\n",
    "            \n",
    "    return collected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631586f-e152-4f4b-8cea-d31f06b2f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read the dictionary_of_urls from the JSON file\n",
    "    with open(\"dictionary_of_urls_1000.json.json\", \"r\") as json_file:\n",
    "        loaded_dictionary = json.load(json_file)    \n",
    "    \n",
    "    websites_to_visit = list(loaded_dictionary)\n",
    "    for web in websites_to_visit:\n",
    "        loaded_dictionary[web].reverse()\n",
    "    \n",
    "    collected_data = collect_website_data(websites_to_visit, loaded_dictionary)\n",
    "    \n",
    "    # Save the collected data to a JSON file\n",
    "    createJSON(\"collected_data.json\", collected_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
